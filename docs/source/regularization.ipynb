{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyUnfold Advanced Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial presents some unfolding examples using additional PyUnfold features.\n",
    "\n",
    "Most notably these include:\n",
    "\n",
    "- Implementing a user-defined prior\n",
    "- Smoothing via spline regularization\n",
    "- Demonstrating the multivariate capabilities using cause groups\n",
    "- Potential pitfalls in using advanced tools\n",
    "\n",
    "But first lets set up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyunfold import iterative_unfold\n",
    "from pyunfold.callbacks import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "np.random.seed(2)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True and Observed Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate some data with the same distributions as in the main tutorial, i.e. a Gaussian sample that is smeared by some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True distribution\n",
    "num_samples = int(1e5)\n",
    "true_samples = np.random.normal(loc=0.0, scale=1.0, size=num_samples)\n",
    "bins = np.linspace(-3, 3, 21)\n",
    "data_true, _ = np.histogram(true_samples, bins=bins)\n",
    "\n",
    "# Observed distribution\n",
    "random_noise = np.random.normal(loc=0.3, scale=0.5, size=num_samples)\n",
    "observed_samples = true_samples + random_noise\n",
    "data_observed, _ = np.histogram(observed_samples, bins=bins)\n",
    "data_observed_err = np.sqrt(data_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(true_samples, bins=bins, histtype='step', lw=3,\n",
    "         alpha=0.7, label='True distribution')\n",
    "plt.hist(observed_samples, bins=bins, histtype='step', lw=3,\n",
    "         alpha=0.7, label='Observed distribution')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Function\n",
    "\n",
    "We'll keep our detection efficiency at 1 for all cause bins for now, again keeping the same response matrix as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiencies\n",
    "efficiencies = np.ones_like(data_observed, dtype=float)\n",
    "efficiencies_err = np.full_like(efficiencies, 0.1, dtype=float)\n",
    "\n",
    "# Response matrix\n",
    "response_hist, _, _ = np.histogram2d(observed_samples, true_samples, bins=bins)\n",
    "response_hist_err = np.sqrt(response_hist)\n",
    "\n",
    "# Scale by efficiency\n",
    "column_sums = response_hist.sum(axis=0)\n",
    "normalization_factor = efficiencies / column_sums\n",
    "\n",
    "response = response_hist * normalization_factor\n",
    "response_err = response_hist_err * normalization_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(response,\n",
    "           origin='lower', \n",
    "           extent=[bins.min(), bins.max(), bins.min(), bins.max()])\n",
    "plt.colorbar(label='$P(E_i|C_{\\mu})$')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Effect bins')\n",
    "plt.title('Normalized response matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Priors\n",
    "\n",
    "The default initial prior is the uniform distribution.\n",
    "We can test other priors by providing a normalized distribution via the `prior` keyword\n",
    "in the `iterative_unfold` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such custom prior is the non-informative Jeffreys' prior.\n",
    "So far we have made no mention of the nature of the causes, simply assigning them bins.\n",
    "To use the Jeffreys' prior (via the `jeffreys_prior` function), we'll need the cause range.\n",
    "Here we assume that the cause range covers three orders of magnitude, $C_{\\mu} \\in [1, 10^3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyunfold.priors import jeffreys_prior, uniform_prior\n",
    "\n",
    "# Cause limits\n",
    "num_causes = len(efficiencies)\n",
    "cause_lim = np.logspace(0, 3, num_causes)\n",
    "\n",
    "# Uniform and Jeffreys' priors\n",
    "uni_prior = uniform_prior(num_causes)\n",
    "jeff_prior = jeffreys_prior(cause_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(cause_lim, jeff_prior, lw=3,\n",
    "         alpha=0.7, label='Jeffreys')\n",
    "plt.step(cause_lim, uni_prior, lw=3,\n",
    "         alpha=0.7, label='Uniform')\n",
    "plt.title('Priors')\n",
    "plt.xlabel(r'Cause Values $C_{\\mu}$')\n",
    "plt.ylabel(r'$P(C_{\\mu})$')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfold\n",
    "\n",
    "Now we can run the unfolding with the Jeffreys' prior and compare to the default as well as the true cause distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running with uniform prior...\")\n",
    "unfolded_results_uni = iterative_unfold(data=data_observed,\n",
    "                                        data_err=data_observed_err,\n",
    "                                        response=response,\n",
    "                                        response_err=response_err,\n",
    "                                        efficiencies=efficiencies,\n",
    "                                        efficiencies_err=efficiencies_err,\n",
    "                                        ts='ks',\n",
    "                                        ts_stopping=0.01,\n",
    "                                        callbacks=[Logger()])\n",
    "\n",
    "print('\\n')\n",
    "print(\"Running with Jeffreys' prior...\")\n",
    "unfolded_results_jeff = iterative_unfold(data=data_observed,\n",
    "                                        data_err=data_observed_err,\n",
    "                                        response=response,\n",
    "                                        response_err=response_err,\n",
    "                                        efficiencies=efficiencies,\n",
    "                                        efficiencies_err=efficiencies_err,\n",
    "                                        prior=jeff_prior,\n",
    "                                        ts='ks',\n",
    "                                        ts_stopping=0.01,\n",
    "                                        callbacks=[Logger()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_midpoints = (bins[1:] + bins[:-1]) / 2\n",
    "plt.hist(true_samples, bins=bins, histtype='step', lw=3,\n",
    "         alpha=0.7,\n",
    "         label='True distribution')\n",
    "\n",
    "plt.errorbar(bin_midpoints, unfolded_results_uni['unfolded'],\n",
    "             yerr=unfolded_results_uni['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='o', ms=5, \n",
    "             label='Unfolded - Uniform Prior')\n",
    "\n",
    "plt.errorbar(bin_midpoints, unfolded_results_jeff['unfolded'],\n",
    "             yerr=unfolded_results_jeff['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5, \n",
    "             label='Unfolded - Jeffreys Prior')\n",
    "\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unfolded distributions are consistent with each other as well as with the true distribution!\n",
    "Thus, our results are robust with respect to these two **smooth** initial priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many experimental setups, we expect the true cause distribution to be fairly smooth.\n",
    "What if we try some bumpy, potentially *non-physical* prior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bumpy random prior\n",
    "bumpy_prior = np.abs(np.random.normal(loc=1, scale=0.35, size=num_causes))\n",
    "bumpy_prior /= bumpy_prior.sum()\n",
    "\n",
    "plt.step(cause_lim, jeff_prior, lw=2,\n",
    "         alpha=0.5, label='Jeffreys')\n",
    "plt.step(cause_lim, uni_prior, lw=2,\n",
    "         alpha=0.5, label='Uniform')\n",
    "plt.step(cause_lim, bumpy_prior, lw=3,\n",
    "         alpha=0.9, label='Bumpy')\n",
    "plt.title('Priors')\n",
    "plt.xlabel(r'Cause Values $C_{\\mu}$')\n",
    "plt.ylabel(r'$P(C_{\\mu})$')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='lower left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running with bumpy prior...\")\n",
    "unfolded_results_bumpy = iterative_unfold(data=data_observed,\n",
    "                                          data_err=data_observed_err,\n",
    "                                          response=response,\n",
    "                                          response_err=response_err,\n",
    "                                          efficiencies=efficiencies,\n",
    "                                          efficiencies_err=efficiencies_err,\n",
    "                                          prior=bumpy_prior,\n",
    "                                          ts='ks',\n",
    "                                          ts_stopping=0.01,\n",
    "                                          callbacks=[Logger()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_midpoints = (bins[1:] + bins[:-1]) / 2\n",
    "plt.hist(true_samples, bins=bins, histtype='step', lw=3,\n",
    "         alpha=0.5,\n",
    "         label='True distribution')\n",
    "\n",
    "plt.errorbar(bin_midpoints, unfolded_results_uni['unfolded'],\n",
    "             yerr=unfolded_results_uni['sys_err'],\n",
    "             alpha=0.5,\n",
    "             elinewidth=1,\n",
    "             capsize=2,\n",
    "             ls='None', marker='o', ms=5, \n",
    "             label='Unfolded - Uniform Prior')\n",
    "\n",
    "plt.errorbar(bin_midpoints, unfolded_results_bumpy['unfolded'],\n",
    "             yerr=unfolded_results_bumpy['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=3,\n",
    "             capsize=4,\n",
    "             ls='None', marker='x', ms=5, \n",
    "             label='Unfolded - Bumpy Prior')\n",
    "\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the non-smooth initial prior has a **strong** influence on the unfolded spectrum. \n",
    "This is one potential pitfall: the initial prior *informs* the procedure as to what properties we think the true distribution has. Unlike for the uniform and Jeffreys cases, the bumpy prior tells the unfolder that the true distribution may be bumpy, so the results are also bumpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "We can try to ameliorate this bumpiness by smoothing or regularizing during the unfolding procedure.\n",
    "Here, we can tune the univariate `SplineRegularizer` routine accessible from the `pyunfold.callbacks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyunfold.callbacks import SplineRegularizer\n",
    "degree = 3\n",
    "smooth = 5e6\n",
    "spline_reg = SplineRegularizer(degree=degree, smooth=smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_bumpy_reg = iterative_unfold(data=data_observed,\n",
    "                                              data_err=data_observed_err,\n",
    "                                              response=response,\n",
    "                                              response_err=response_err,\n",
    "                                              efficiencies=efficiencies,\n",
    "                                              efficiencies_err=efficiencies_err,\n",
    "                                              prior=bumpy_prior,\n",
    "                                              ts='ks',\n",
    "                                              ts_stopping=0.01,\n",
    "                                              callbacks=[Logger(), spline_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_midpoints = (bins[1:] + bins[:-1]) / 2\n",
    "plt.hist(true_samples, bins=bins, histtype='step', lw=3,\n",
    "         alpha=0.7,\n",
    "         label='True distribution')\n",
    "\n",
    "plt.errorbar(bin_midpoints, unfolded_results_bumpy['unfolded'],\n",
    "             yerr=unfolded_results_bumpy['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5, \n",
    "             label='No Regularization')\n",
    "\n",
    "plt.errorbar(bin_midpoints, unfolded_results_bumpy_reg['unfolded'],\n",
    "             yerr=unfolded_results_bumpy_reg['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5, \n",
    "             label='With Regularization')\n",
    "\n",
    "plt.title('Bumpy Initial Priors')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized unfolded result is indeed more consistent with the true distribution, but at the cost of taking a couple of extra iterations to converge, and thus has slightly larger uncertainties due to more mixing.\n",
    "\n",
    "For this example, a strong regularization parameter (`smooth = 5e6`) was needed to return consistent unfolded results. Assuming a smooth initial prior is the proper way to avoid this potential issue, unless one does indeed have overriding assumptions regarding smoothness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cause Groups\n",
    "\n",
    "As a final demonstration of advanced capabilities in PyUnfold, we present the generalization to multivariate unfolding.\n",
    "\n",
    "To illustrate this idea, we consider a set of effects originating from two different **cause types** having their own ranges. We can assign a new superscript $i$ to denote these types: \n",
    "\n",
    "$$\n",
    "C_{\\mu}^{i} \\text{ where } i \\in [1,2]\n",
    "$$\n",
    "\n",
    "and the subscript $\\mu$ runs over the respective number of causes in each type $\\, n_{C1}$ and $\\, n_{C2}$.\n",
    "\n",
    "But since the PyUnfold doesn't care how we label the bins, we can simply redefine our cause index $\\mu$ to run over a larger index range. Hence, \n",
    "\n",
    "$$\n",
    "C_{\\mu}^i \\rightarrow C_{\\mu} \\text{ where } \\mu \\in [1, \\, n_{C1} + n_{C2}]\n",
    "$$\n",
    "\n",
    "Thus, given a general multidimensional ($i>1$) response matrix, we can effectively *unroll* it onto a two dimensional array and still use PyUnfold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "It's probably easier to use a visual example. Here we use two identical cause groups, simply copying our response matrix along the cause axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response with two groups\n",
    "response_hist_groups = np.concatenate((response_hist, response_hist), axis=1)\n",
    "response_hist_groups_err = np.sqrt(response_hist_groups)\n",
    "\n",
    "# Efficiencies with two groups\n",
    "efficiencies_groups = np.ones(response_hist_groups.shape[1], dtype=float)\n",
    "efficiencies_groups_err = np.full_like(efficiencies_groups, 0.1, dtype=float)\n",
    "\n",
    "# Scale by efficiency\n",
    "column_sums_groups = response_hist_groups.sum(axis=0)\n",
    "normalization_factor_groups = efficiencies_groups / column_sums_groups\n",
    "\n",
    "# Response matrix with two groups\n",
    "response_groups = response_hist_groups * normalization_factor_groups\n",
    "response_groups_err = response_hist_groups_err * normalization_factor_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(response_groups,\n",
    "           origin='lower',\n",
    "           aspect='0.5',\n",
    "           extent=[2*bins.min(), 2*bins.max(), 2*bins.min(), 2*bins.max()])\n",
    "plt.colorbar(label='$P(E_i|C_{\\mu})$')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Effect bins')\n",
    "plt.title('Normalized response matrix - groups')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two (identical) cause groups are *unrolled* clearly along the abscissa. Since the unfolding method is cause agnostic, we can perform an unfolding, remembering that we've kept the same observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups = iterative_unfold(data=data_observed,\n",
    "                                           data_err=data_observed_err,\n",
    "                                           response=response_groups,\n",
    "                                           response_err=response_groups_err,\n",
    "                                           efficiencies=efficiencies_groups,\n",
    "                                           efficiencies_err=efficiencies_groups_err,\n",
    "                                           ts='ks',\n",
    "                                           ts_stopping=0.01,\n",
    "                                           callbacks=[Logger()])\n",
    "\n",
    "# Extend range of bins\n",
    "bin_groups = np.linspace(-6, 6, 41)\n",
    "bin_midpoints_groups = (bin_groups[1:] + bin_groups[:-1]) / 2\n",
    "\n",
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups['unfolded'],\n",
    "             yerr=unfolded_results_groups['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5)\n",
    "\n",
    "plt.title('Group Unfolding')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "#plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the result is two equal copies of the causes. This makes sense because in our example we have simply considered two *identical* groups of causes, so they should contribute identically to producing the measured effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Priors with Groups\n",
    "\n",
    "What if we want to use the Jeffreys' prior? In this case, we have to setup the `prior` input to contain the priors we want for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup group Jeffreys' prior\n",
    "prior_jeff_groups = np.concatenate([jeffreys_prior(cause_lim), jeffreys_prior(cause_lim)])\n",
    "prior_jeff_groups /= prior_jeff_groups.sum()\n",
    "\n",
    "plt.step(bin_midpoints_groups, prior_jeff_groups, lw=2,\n",
    "         alpha=0.5)\n",
    "plt.title('Group Jeffreys Priors')\n",
    "plt.xlabel(r'Cause Bins')\n",
    "plt.ylabel(r'$P(C_{\\mu})$')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups_jeff = iterative_unfold(data=data_observed,\n",
    "                                                data_err=data_observed_err,\n",
    "                                                response=response_groups,\n",
    "                                                response_err=response_groups_err,\n",
    "                                                efficiencies=efficiencies_groups,\n",
    "                                                efficiencies_err=efficiencies_groups_err,\n",
    "                                                prior=prior_jeff_groups,\n",
    "                                                ts='ks',\n",
    "                                                ts_stopping=0.01,\n",
    "                                                callbacks=[Logger()])\n",
    "\n",
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups_jeff['unfolded'],\n",
    "             yerr=unfolded_results_groups_jeff['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5)\n",
    "\n",
    "plt.title(\"Group Unfolding - Jeffreys' Prior\")\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we recover two copies of the causes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with Groups\n",
    "\n",
    "In general, groups of causes do not share continuity of the cause axis. \n",
    "In the above examples, the cause arrays are stacked next to each other, so while the unfolding method doesn't care about the cause definitions, the default regularization smooths over **all** cause bins without regard to types.\n",
    "\n",
    "PyUnfold implements group regularization, where smoothing of the unfolded distributions is performed only within designated cause types.\n",
    "This is done by providing the `SplineRegularizer` function a `groups` list, defining the corresponding group numbers for each cause bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 4\n",
    "smooth = 10\n",
    "\n",
    "# Here we know we have two copies of the causes\n",
    "n_c1 = int(response_groups.shape[1]/2)\n",
    "n_c2 = int(response_groups.shape[1]/2)\n",
    "\n",
    "groups = [0]*n_c1 + [1]*n_c2\n",
    "print(\"Group definitions: {}\".format(groups))\n",
    "\n",
    "group_reg = SplineRegularizer(degree=3, smooth=1.25, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups_reg = iterative_unfold(data=data_observed,\n",
    "                                               data_err=data_observed_err,\n",
    "                                               response=response_groups,\n",
    "                                               response_err=response_groups_err,\n",
    "                                               efficiencies=efficiencies_groups,\n",
    "                                               efficiencies_err=efficiencies_groups_err,\n",
    "                                               ts='ks',\n",
    "                                               ts_stopping=0.01,\n",
    "                                               callbacks=[Logger(), group_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups_reg['unfolded'],\n",
    "             yerr=unfolded_results_groups_reg['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5)\n",
    "\n",
    "plt.title('Group Unfolding')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for this simple examle, everything looks fine.\n",
    "\n",
    "However, if we consider a more complicated example, we'll see the power of these implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some more fake data by keeping our two-type response matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True distribution\n",
    "group_data_true = np.concatenate((np.linspace(10, 100, n_c1), np.linspace(1, 10, n_c2)))\n",
    "# Observed data, no smearing\n",
    "group_data_observed = response_groups.dot(group_data_true)\n",
    "group_data_observed_err = np.sqrt(group_data_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1, aspect=0.1)\n",
    "plt.step(bin_midpoints_groups, group_data_true, lw=3,\n",
    "         alpha=0.7, label='True distribution')\n",
    "plt.title('Cause Distribution')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "plt.subplot(1, 2, 2, aspect=0.035)\n",
    "plt.step(bin_midpoints, group_data_observed, lw=3,\n",
    "         alpha=0.7, label='Observed distribution')\n",
    "plt.title('Effects Distribution')\n",
    "plt.xlabel('Effect bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.subplots_adjust(wspace = .5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup all cause spline\n",
    "degree = 3\n",
    "smooth = 5e6\n",
    "spline_reg = SplineRegularizer(degree=degree, smooth=smooth)\n",
    "\n",
    "# Setup group spline\n",
    "degree = 3\n",
    "smooth = 10\n",
    "group_reg = SplineRegularizer(degree=3, smooth=1.25, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups_default_reg = iterative_unfold(data=group_data_observed,\n",
    "                                                       data_err=group_data_observed_err,\n",
    "                                                       response=response_groups,\n",
    "                                                       response_err=response_groups_err,\n",
    "                                                       efficiencies=efficiencies_groups,\n",
    "                                                       efficiencies_err=efficiencies_groups_err,\n",
    "                                                       ts='ks',\n",
    "                                                       ts_stopping=0.01,\n",
    "                                                       callbacks=[Logger(), spline_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups_group_reg = iterative_unfold(data=group_data_observed,\n",
    "                                                     data_err=group_data_observed_err,\n",
    "                                                     response=response_groups,\n",
    "                                                     response_err=response_groups_err,\n",
    "                                                     efficiencies=efficiencies_groups,\n",
    "                                                     efficiencies_err=efficiencies_groups_err,\n",
    "                                                     ts='ks',\n",
    "                                                     ts_stopping=0.01,\n",
    "                                                     callbacks=[Logger(), group_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(bin_midpoints_groups, group_data_true, lw=3,\n",
    "         alpha=0.7, label='True Distribution')\n",
    "\n",
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups_default_reg['unfolded'],\n",
    "             yerr=unfolded_results_groups_default_reg['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='o', ms=5,\n",
    "             label='Default Regularization')\n",
    "\n",
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups_group_reg['unfolded'],\n",
    "             yerr=unfolded_results_groups_group_reg['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5,\n",
    "             label='Group Regularization')\n",
    "\n",
    "plt.title('Group Unfolding')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, what's going on here?\n",
    "\n",
    "So there are actually two problems here. \n",
    "\n",
    "1. It's clear that the default regularization tries to connect the two cause groups in a smooth manner.\n",
    "2. And while the group regularization is doing its best to smooth each group individually, it's clear we are getting copies again. This is due to the **prior** assumption that all causes have equal probabilities.\n",
    "\n",
    "Let's redo this by giving a strong preference in the prior to one of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat prior to each group, but one has stronger preference.\n",
    "flat_pref_prior = np.concatenate((8.*np.ones(n_c1), np.ones(n_c1)))\n",
    "flat_pref_prior /= flat_pref_prior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups_group_reg_prior = iterative_unfold(data=group_data_observed,\n",
    "                                                           data_err=group_data_observed_err,\n",
    "                                                           response=response_groups,\n",
    "                                                           response_err=response_groups_err,\n",
    "                                                           efficiencies=efficiencies_groups,\n",
    "                                                           efficiencies_err=efficiencies_groups_err,\n",
    "                                                           prior=flat_pref_prior,\n",
    "                                                           ts='ks',\n",
    "                                                           ts_stopping=0.01,\n",
    "                                                           callbacks=[Logger(), group_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(bin_midpoints_groups, group_data_true, lw=3,\n",
    "         alpha=0.7, label='True Distribution')\n",
    "\n",
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups_group_reg_prior['unfolded'],\n",
    "             yerr=unfolded_results_groups_group_reg_prior['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5,\n",
    "             label='Group Regularization')\n",
    "\n",
    "plt.title('Group Unfolding')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're starting to get something that looks right.\n",
    "\n",
    "However, we are still using two identical copies of the response matrix which means that we're considering the same sets of causes, just split up into two groups.\n",
    "\n",
    "This also demonstrates another potential issue with doing group unfolding: if there is high degeneracy between the respective response functions, then the unfolding results will be highly dependent on intial priors.\n",
    "\n",
    "The solution to this is to ensure that the cause group response matrices have different structure, for example by having different normalizations (efficiency). We demonstrate this below to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response with two groups having different structures\n",
    "n_c1 = len(response_hist)\n",
    "n_c2 = 5\n",
    "response_hist_groups = np.concatenate((response_hist, response_hist[:,0:n_c2]), axis=1)\n",
    "response_hist_groups_err = np.sqrt(response_hist_groups)\n",
    "\n",
    "# Efficiencies with two groups\n",
    "efficiencies_groups = np.ones(response_hist_groups.shape[1], dtype=float)\n",
    "efficiencies_groups_err = np.full_like(efficiencies_groups, 0.1, dtype=float)\n",
    "# Make the second group 25% efficient\n",
    "efficiencies_groups[0:n_c1] *= 1\n",
    "efficiencies_groups[n_c1:-1] *= 0.25\n",
    "efficiencies_groups_err = np.full_like(efficiencies_groups, 0.1, dtype=float)\n",
    "\n",
    "# Scale by efficiency\n",
    "column_sums_groups = response_hist_groups.sum(axis=0)\n",
    "normalization_factor_groups = efficiencies_groups / column_sums_groups\n",
    "\n",
    "# Response matrix with two groups\n",
    "response_groups = response_hist_groups * normalization_factor_groups\n",
    "response_groups_err = response_hist_groups_err * normalization_factor_groups\n",
    "\n",
    "\n",
    "groups = [0]*n_c1 + [1]*n_c2\n",
    "print(\"Group definitions: {}\".format(groups))\n",
    "\n",
    "group_reg = SplineRegularizer(degree=3, smooth=1.25, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(response_groups,\n",
    "           origin='lower',\n",
    "           aspect='0.5',\n",
    "           extent=[2*bins.min(), 2*bins.max(), 2*bins.min(), 2*bins.max()])\n",
    "plt.colorbar(label='$P(E_i|C_{\\mu})$')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Effect bins')\n",
    "plt.title('Normalized response matrix - groups')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regenerate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True distribution\n",
    "group_data_true = np.concatenate((np.linspace(10, 100, n_c1), np.linspace(1, 10, n_c2)))\n",
    "# Observed data, no smearing\n",
    "group_data_observed = response_groups.dot(group_data_true)\n",
    "group_data_observed_err = np.sqrt(group_data_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat prior to each group, but one has slightly more preference.\n",
    "#flat_pref_prior = np.concatenate((1.*np.ones(n_c1), np.ones(n_c2)))\n",
    "#flat_pref_prior /= flat_pref_prior.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded_results_groups_group_reg = iterative_unfold(data=group_data_observed,\n",
    "                                                     data_err=group_data_observed_err,\n",
    "                                                     response=response_groups,\n",
    "                                                     response_err=response_groups_err,\n",
    "                                                     efficiencies=efficiencies_groups,\n",
    "                                                     efficiencies_err=efficiencies_groups_err,\n",
    "                                                     #prior=flat_pref_prior,\n",
    "                                                     ts='ks',\n",
    "                                                     ts_stopping=0.01,\n",
    "                                                     callbacks=[Logger(), group_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend range of bins\n",
    "bin_groups = np.linspace(-6, 6, n_c1+n_c2+1)\n",
    "bin_midpoints_groups = (bin_groups[1:] + bin_groups[:-1]) / 2\n",
    "\n",
    "plt.step(bin_midpoints_groups, group_data_true, lw=3,\n",
    "         alpha=0.7, label='True Distribution')\n",
    "\n",
    "plt.errorbar(bin_midpoints_groups, unfolded_results_groups_group_reg['unfolded'],\n",
    "             yerr=unfolded_results_groups_group_reg['sys_err'],\n",
    "             alpha=0.8,\n",
    "             elinewidth=2,\n",
    "             capsize=3,\n",
    "             ls='None', marker='x', ms=5,\n",
    "             label='Group Regularization')\n",
    "\n",
    "plt.title('Group Unfolding')\n",
    "plt.xlabel('Cause bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend(loc='best', frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the groups are differentiable in terms of response structure and efficiencies, we obtain reasonable unfolding results even using the default uniform prior across all bins!\n",
    "\n",
    "This simply illustrates the need for proper understanding of one's data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
